{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a742309",
   "metadata": {},
   "source": [
    "# AI Agents Introduction\n",
    "## RAG\n",
    "- (R)etrieval - context information found in a vector database and provided to an LLM - Actor: __Vector Database__\n",
    "- (A)ugmented - this context is _enriched_ with metadata - Actor: __Vector Database__\n",
    "- (G)eneration - LLM generates the output from the context according to the prompt requirements - Actor: __LLM__\n",
    "\n",
    "So LLMs (and AI at all) plays role only in the last step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4992b0",
   "metadata": {},
   "source": [
    "# Restaurant\n",
    "\n",
    "- Given a restaurant making swiss food\n",
    "- A customer from Hungary wants hungarian food\n",
    "- The chef has no idea, how to make it. <br>\n",
    "What can he do?<br><br><br>\n",
    "\n",
    "- Guess and make. Terrible result. -> in AI: __Hallucination__\n",
    "- Directly say no -> bad impression, but still better than hallucination\n",
    "- Read a __cooking book__ -> (hopefully) delicious food made, this would be __RAG__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304b73ed",
   "metadata": {},
   "source": [
    "# Customer Support\n",
    "Q: What is the company's return policy after Black Friday?\n",
    "\n",
    "## Without RAG\n",
    "A: Generally companies offer 30-day return but can be differences\n",
    "\n",
    "## With RAG\n",
    "have an AI assistant with LLM and vector data base with company data<br>\n",
    "A: According to our policy (document xyz), the return policy is..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5850c1",
   "metadata": {},
   "source": [
    "# RAG Benefits\n",
    "- Cost saving\n",
    "- Accuracy improvement (hallucination)\n",
    "- Flexible update (LLM: 6 Month updat period, RAG can be nearly real-time)\n",
    "- Regulatory and Audit Complience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc2bbc",
   "metadata": {},
   "source": [
    "# Prompt engineering vs. Fine tuning vs. RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6718399e",
   "metadata": {},
   "source": [
    "## Prompt engineering\n",
    "- Use the __base LLM__ only\n",
    "- Instructions\n",
    "- -- Role\n",
    "- -- Context\n",
    "- -- Question\n",
    "- -- Output format\n",
    "\n",
    "Pros:\n",
    "- no technical expertise needed\n",
    "- instant results\n",
    "- no training costs\n",
    "- works with any LLM\n",
    "\n",
    "Cons:\n",
    "- limited by the model's base knowledge\n",
    "- inconsistent result (repeating the same prompt can give contradictory results)\n",
    "- token limit restricts complexity\n",
    "- cannot add new knowledge\n",
    "\n",
    "Best for:\n",
    "- small scale, quick application\n",
    "- generic purpose tasks\n",
    "- quick prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11187bf6",
   "metadata": {},
   "source": [
    "## Fine Tuning\n",
    "e.g. ChatBot for a specific company\n",
    "\n",
    "- Base LLM\n",
    "- Add company-specific training data\n",
    "- -> LLM weights are modified\n",
    "\n",
    "How:\n",
    "- prepare domain specific training data\n",
    "- train the model on the data\n",
    "- change is permanent, weights modified -> specific version of the LLM\n",
    "\n",
    "Pros:\n",
    "- deeply specialized knowledge\n",
    "- consistend behavior\n",
    "- _no prompt engineering needed_\n",
    "\n",
    "Cons:\n",
    "- expensive (data collection, preprocessing, GPU/TPUs)\n",
    "- requires ML/GenAI expertise\n",
    "- regular retraining required -> always updates the base LLM\n",
    "\n",
    "Best for:\n",
    "- specific style\n",
    "- high volume domain specific data\n",
    "- accuracy critical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da47f343",
   "metadata": {},
   "source": [
    "# RAG\n",
    "specific data is not trained to the model, but stored as external data\n",
    "\n",
    "How:\n",
    "- all knowledge is stored in __vector database__\n",
    "- retrieving relevant data (context) for every query separate\n",
    "- LLM generates the answer from the provided context\n",
    "\n",
    "Pros:\n",
    "- up-to-date information\n",
    "- base LLM is not modified (spare training costs)\n",
    "- high accuracy (but not as high as at fine tuning)\n",
    "- can handle private/proprietary data (vectir database can be on-premise)\n",
    "\n",
    "Cons:\n",
    "- infrastructure (vector database)\n",
    "- retrieval quality affects result\n",
    "- context window limitation\n",
    "\n",
    "Best for:\n",
    "- real time information\n",
    "- regulatory comstraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b8e36",
   "metadata": {},
   "source": [
    "# RAG Core Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625b4f97",
   "metadata": {},
   "source": [
    "## Document ingestion and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85065eef",
   "metadata": {},
   "source": [
    "- collect data (.pdf or other files, web sites, databases, API calls, etc)\n",
    "- data can be text, image, sound, etc\n",
    "\n",
    "# Chunks\n",
    "LLMs have limit regarding the size of the prompt (maximum tokens).<br>\n",
    "We cannot give endless long text as input<br>\n",
    "The context we retrieve from the vector database is passed to the LLM\n",
    "\n",
    "-> it must be short and consciese\n",
    "\n",
    "YOu can try to send a 1000 pages book as context, some LLMs can handle it, some not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b2d78",
   "metadata": {},
   "source": [
    "# Splitter\n",
    "Converts the data into small pieces.\n",
    "\n",
    "But this is still the original data (bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e9a68f",
   "metadata": {},
   "source": [
    "## Embedding model\n",
    "\n",
    "Converts the original data into __vectors__\n",
    "\n",
    "\"Abcde fghi jklmn\" -> [0.234, 0.09884, 0,8443, ... , 0.532, 0.346236]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4655433",
   "metadata": {},
   "source": [
    "## These vectors are stored in the __Vector Database__\n",
    "but not alone. We store\n",
    "- the vector\n",
    "- the original data\n",
    "- metadata (information source, etc)\n",
    "\n",
    "in one retrievable unit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e7e7d",
   "metadata": {},
   "source": [
    "# Query Processing Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cadb71",
   "metadata": {},
   "source": [
    "## Imput query\n",
    "This is a prompt without context (Q0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a883fad7",
   "metadata": {},
   "source": [
    "# Vectorize\n",
    "Using the __same empbedding model__ as at data processing (V0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816c51ae",
   "metadata": {},
   "source": [
    "## Find stored chunks\n",
    "Vector database have plenty of records with vectors V1, V2, ... Vn\n",
    "\n",
    "Aim: find the __closest__ vectors in the database to vector V0.<br>\n",
    "Normally we tell the VDB, maximum how many matches we want.\n",
    "\n",
    "- We find Vx, Vy and Vz as the closest ones\n",
    "- So we return the original data chunk Dx, Dy, Dz together with the metadata Mx, My, Mz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7554178b",
   "metadata": {},
   "source": [
    "# Generation Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fd63ee",
   "metadata": {},
   "source": [
    "## Modify the Prompt\n",
    "\n",
    "We have the original prompt (user query) Q0.\n",
    "\n",
    "Now we __enhance__ it with the data we found, e.g.\n",
    "\n",
    "\"... original prompt ...<br>\n",
    "Use the following information to create your answer:<br>\n",
    "Dx (Mx), Dy (My), Dz (Mz)<br>\n",
    "Format the answer as the following: ...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9ee5bc",
   "metadata": {},
   "source": [
    "## Send to LLM\n",
    "We send the augmented (enriched) prompt to an arbitrary LLM modela nd get the formatted answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06380747",
   "metadata": {},
   "source": [
    "# UV\n",
    "\n",
    "Not related too much, but uv is a fast replacement of pip.\n",
    "\n",
    "### Install\n",
    "`> pip install uv`\n",
    "\n",
    "### Initialize a project\n",
    "/in/my/project/folder `> uv init`\n",
    "\n",
    "it creates some convenience files fro a Python project, but doe not create the environment, so before of after that also must be created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3347ea",
   "metadata": {},
   "source": [
    "# __Preparations__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18c7d21",
   "metadata": {},
   "source": [
    "# Langchain\n",
    "\n",
    "We will use Langchain for different purposes.<br>\n",
    "Langchain components:\n",
    "- LangGraph Platform\n",
    "- LangChain\n",
    "- LangGraph\n",
    "- LangSmith\n",
    "- Integrations\n",
    "\n",
    "As the name suggests, Integration can be used for data ingestion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562ebab3",
   "metadata": {},
   "source": [
    "## Python libraries we need\n",
    "- langchain\n",
    "- langchain-groq\n",
    "- faiss-cpu\n",
    "- tiktoken\n",
    "- langchain-community\n",
    "- langchain-openai\n",
    "- chromadb\n",
    "- sentence-transformers\n",
    "- pypdf\n",
    "- python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5958d649",
   "metadata": {},
   "source": [
    "## Environment variables\n",
    "create a file `.env` to store environment variables used by `dotenv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705bef9b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
